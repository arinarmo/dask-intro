{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 9 - Dask\n",
    "\n",
    "_175904 - Jorge III Altamirano Astorga_\n",
    "\n",
    "## Ejercicio/Tarea\n",
    "\n",
    "Aprovecha la capacidad de Dask para realizar cómputo en paralelo para ajustar un modelo para predecir la proporción de propina de un viaje. Realiza búsqueda de hiperparámetros en grid con cross validation. Puedes usar funciones de scikit learn. Recuerda usar el decorador `delayed` para ejecutar en paralelo.\n",
    "\n",
    "* ¿Qué tan rápido es buscar en paralelo comparado con una búsqueda secuencial en python?\n",
    "\n",
    "Haz lo mismo que arriba, pero utilizando la biblioteca Dask-ML http://dask-ml.readthedocs.io/en/latest/ \n",
    "\n",
    "_Respuesta: es más lento paralelizar que hacerlo secuencialmente, dado el volumen (pequeño de datos) como se detalla en la siguiente respuesta a la pregunta que es pequeño._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 9198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2015-01-05 23:35:02</td>\n",
       "      <td>2015-01-05 23:25:15</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2015-01-06 15:22:12</td>\n",
       "      <td>2015-01-06 15:11:45</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2015-01-08 08:31:23</td>\n",
       "      <td>2015-01-08 08:22:12</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2015-01-08 12:35:54</td>\n",
       "      <td>2015-01-08 12:26:26</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_type  fare_amount  passenger_count  taxi_id  tip_amount  \\\n",
       "1        A          9.0                1        1        0.00   \n",
       "2        A          7.5                1        1        1.00   \n",
       "3        A          8.5                1        1        1.00   \n",
       "4        A          7.5                1        1        1.66   \n",
       "\n",
       "  tpep_dropoff_datetime tpep_pickup_datetime  trip_distance  \n",
       "1   2015-01-05 23:35:02  2015-01-05 23:25:15           1.81  \n",
       "2   2015-01-06 15:22:12  2015-01-06 15:11:45           0.96  \n",
       "3   2015-01-08 08:31:23  2015-01-08 08:22:12           1.90  \n",
       "4   2015-01-08 12:35:54  2015-01-08 12:26:26           1.00  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import *\n",
    "import pandas as pd\n",
    "\n",
    "trips_pd = pd.read_csv(\"../data/trips.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    trips_pd.iloc[:, [1,2,3,7]],\n",
    "    trips_pd.tip_amount,\n",
    "    test_size=0.3,\n",
    "    random_state=175904)\n",
    "print(\"Number of records: %d\"%trips_pd.shape[0])\n",
    "trips_pd[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.42 s, sys: 5.73 s, total: 8.15 s\n",
      "Wall time: 5.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "grid_params = { \n",
    "    \"lasso0__eps\": [1e-3, 1e-4, 1e-5],\n",
    "    \"lasso0__n_alphas\": [100, 250, 500]}\n",
    "lasso0 = LassoCV(max_iter=1000, n_alphas=300, n_jobs=15, random_state=175904)\n",
    "pipe0  = Pipeline([(\"lasso0\", lasso0)])\n",
    "grid = GridSearchCV(estimator=pipe0, \n",
    "                    param_grid=grid_params, \n",
    "                    n_jobs=14, scoring=\"neg_mean_squared_error\")\n",
    "gridm = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -3.360575 with metric 'neg_mean_squared_error'\n",
      "The best parameters are: {'lasso0__eps': 0.0001, 'lasso0__n_alphas': 500}\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: %f with metric '%s'\\nThe best parameters are: %s\"%(\n",
    "    gridm.score(X_test, y_test), \n",
    "    grid.scoring,\n",
    "    gridm.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.61 ms, sys: 6.93 ms, total: 8.54 ms\n",
      "Wall time: 682 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.90660838, 0.83992721, 0.92645592, ..., 0.79878273, 0.82947737,\n",
       "       0.65123205])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gridm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2015-01-03 01:37:02</td>\n",
       "      <td>2015-01-03 01:17:32</td>\n",
       "      <td>6.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-01-05 23:35:02</td>\n",
       "      <td>2015-01-05 23:25:15</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-06 15:22:12</td>\n",
       "      <td>2015-01-06 15:11:45</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-01-08 08:31:23</td>\n",
       "      <td>2015-01-08 08:22:12</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_type  fare_amount  passenger_count  taxi_id  tip_amount  \\\n",
       "0        A         22.0                1        1         4.6   \n",
       "1        A          9.0                1        1         0.0   \n",
       "2        A          7.5                1        1         1.0   \n",
       "3        A          8.5                1        1         1.0   \n",
       "\n",
       "  tpep_dropoff_datetime tpep_pickup_datetime  trip_distance  \n",
       "0   2015-01-03 01:37:02  2015-01-03 01:17:32           6.90  \n",
       "1   2015-01-05 23:35:02  2015-01-05 23:25:15           1.81  \n",
       "2   2015-01-06 15:22:12  2015-01-06 15:11:45           0.96  \n",
       "3   2015-01-08 08:31:23  2015-01-08 08:22:12           1.90  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask_ml.model_selection as dms\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "import numpy as np\n",
    "from dask import delayed\n",
    "\n",
    "dask.set_options(scheduler='threads')\n",
    "client = Client(\"jupyter.corp.penoles.mx:8786\")\n",
    "\n",
    "trips = dd.read_csv(\"file:/tmp/trips.csv\")\n",
    "dX_train, dX_test, dy_train, dy_test = (\n",
    "    dd.from_pandas(X_train, npartitions=3),\n",
    "    dd.from_pandas(X_test, npartitions=3),\n",
    "    dd.from_pandas(y_train, npartitions=3),\n",
    "    dd.from_pandas(y_test, npartitions=3)\n",
    ")\n",
    "\n",
    "trips.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.9 ms, sys: 12 ms, total: 93.9 ms\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "grid_params = { \n",
    "    \"lasso0__eps\": [1e-3, 1e-4, 1e-5],\n",
    "    \"lasso0__n_alphas\": [100, 250, 500]}\n",
    "\n",
    "lasso0 = LassoCV(max_iter=1000, n_alphas=300, n_jobs=15, random_state=175904)\n",
    "pipe0  = Pipeline([(\"lasso0\", lasso0)])\n",
    "dgrid = dms.GridSearchCV(estimator=pipe0, \n",
    "                    param_grid=grid_params, \n",
    "                    n_jobs=14, scoring=\"neg_mean_squared_error\")\n",
    "dgridm = dgrid.fit(dX_train, dy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -3.360575 with metric 'neg_mean_squared_error'\n",
      "The best parameters are: {'lasso0__eps': 0.0001, 'lasso0__n_alphas': 500}\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: %f with metric '%s'\\nThe best parameters are: %s\"%(\n",
    "    gridm.score(dX_test, dy_test), \n",
    "    grid.scoring,\n",
    "    gridm.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 267 ms, sys: 17 ms, total: 284 ms\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_hat = dX_test.map_partitions(dgridm.predict)\n",
    "y_hat.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Cómo se comparan los tiempos de ejecución de tu búsqueda con la de Dask ML?\n",
    "\n",
    "_Respuesta: el costo de ejecución en paralelo (dada la distribución de los datos y paralelización) es mayor al costo que tiene procesarlos en un solo nodo. Por lo tanto toma ligeramente más tiempo dicho \"overhead\" que si lo hicieramos secuencialmente en un solo nodo._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**\n",
    "\n",
    "Haz lo mismo utilizando Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Spark v2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark, sys, os, re\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.worker.cleanup.appDataTtl\", 24*60*60)\n",
    "conf.set(\"spark.worker.cleanup.enabled\", True)\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.driver.cores\", 14)\n",
    "conf.set(\"spark.driver.memoryOverhead\", 0.9)\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.cores\", 14)\n",
    "conf.set(\"spark.jars\", \n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar,\" +\n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/commons-cli-1.2.jar,\"+\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar,\" +\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar\"\n",
    "        )\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "### get they creds to login to AWS :-)\n",
    "HOME = os.environ[\"HOME\"]\n",
    "aws_id, aws_key = (None, None)\n",
    "with open(HOME+\"/.aws/credentials\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"aws_access_key_id\" in line:\n",
    "            aws_id = re.sub(\"^.*aws_access_key_id\\s*=\\s*\", \"\", line)\n",
    "        elif \"aws_secret_access_key\" in line:\n",
    "            aws_key = re.sub(\"^.*aws_secret_access_key\\s*=\\s*\", \"\", line)\n",
    "### end getting keys\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", aws_id)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", aws_key)\n",
    "aws_id, aws_key = (None, None)\n",
    "# conf.set(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.5.3\")\n",
    "sc = SparkContext(master = \"spark://jupyter.corp.penoles.mx:7077\", \n",
    "                  sparkHome=\"/usr/local/spark/\", \n",
    "                  appName=\"tarea-8-dask\", conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "print(\"Running Spark v%s\"%sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Cómo se comparan los tiempos de ejecución de Spark vs Dask?\n",
    "\n",
    "Usa los datos en s3://dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.csv\n",
    "\n",
    "_Respuesta: es más ágil en datos de gran escala solamente Spark vs Dask. Sin embargo, tiene peores resultados en el error, y es menos controlable que el maduro (en comparación) scikit-learn._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to pull parquet in local HDFS instead of downloading CSV in S3... FOUND IT!!!\n",
      "CPU times: user 29.8 ms, sys: 9.6 ms, total: 39.4 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "yellow = yellow_csv = None\n",
    "try:\n",
    "    print(\"Trying to pull parquet in local HDFS instead of downloading CSV in S3...\", end=\"\")\n",
    "    yellow = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/tmp/yellow.parquet\")\n",
    "    print(\" FOUND IT!!!\")\n",
    "except:\n",
    "    print(\"\\n\\nI didn't find the parquet. Then we're downloading it... (this will take a while)\")\n",
    "    yellow_csv = spark.read.csv(\"s3a://dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.csv\", header=True)\n",
    "    print(\"Writing parquet...\")\n",
    "    yellow_csv.write.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/tmp/yellow.parquet\", mode=\"overwrite\")\n",
    "    print(\"Reading parquet...\")\n",
    "    yellow = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/tmp/yellow.parquet\")\n",
    "    pass\n",
    "yellow_csv = None\n",
    "yellow.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|   pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       1| 2015-01-16 04:18:49|  2015-01-16 04:36:07|              3|        10.40|-73.983985900878906|40.721401214599609|         1|                 N|      -73.861328125|    40.76806640625|           1|         30|  0.5|    0.5|         5|           0|                  0.3|        36.3|\n",
      "|       1| 2015-01-16 04:18:50|  2015-01-16 04:29:32|              4|         2.20|-73.988754272460938|40.731380462646484|         1|                 N|-73.990806579589844|40.750751495361328|           1|         10|  0.5|    0.5|      2.25|           0|                  0.3|       13.55|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellow.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12748986"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellow.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformaciones\n",
    "\n",
    "Transformaciones necesarias para trabajar con Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellow = yellow \\\n",
    "    .withColumn(\"fare_amount\", yellow.fare_amount.cast(DoubleType())) \\\n",
    "    .withColumn(\"passenger_count\", yellow.passenger_count.cast(IntegerType())) \\\n",
    "    .withColumn(\"VendorID\", yellow.VendorID.cast(IntegerType())) \\\n",
    "    .withColumn(\"trip_distance\", yellow.trip_distance.cast(DoubleType())) \\\n",
    "    .withColumn(\"tip_amount\", yellow.tip_amount.cast(DoubleType())) \\\n",
    "    .drop(\"label\", \"features\")\n",
    "spark_si = StringIndexer(inputCol=\"tip_amount\", outputCol=\"label\")\n",
    "yellow = spark_si.fit(yellow).transform(yellow).cache()\n",
    "yellow.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación Spark vs Dask vs Secuencial\n",
    "\n",
    "Dado que tenemos un número mucho mayor de registros en S3, hago un sample aleatorio de los datos que están. Sé que son diferentes datos, y pudieran afectar la predicción: pero lo considero no significativo. Por eso creo el subset `yellow_sm` con un poco más de los 9198 que contiene el set de trips usados en Dask y Secuencial.\n",
    "\n",
    "Esto coincide con mis observaciones: realizar el paralelismo toma tiempo y recursos. \n",
    "\n",
    "Por lo tanto, de manera preliminar puedo afirmar que: vale la pena paralelizar siempre y cuando sean datos suficientes. Además, no vale la pena utilizar para programar rápidamente en Spark, dado que requiere que se hagan los pasos previos; por ende, no vale la pena hacer el esfuerzo para datos en pequeña escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 94890\n"
     ]
    }
   ],
   "source": [
    "yellow_sm = yellow.sample(fraction=0.0075, seed=175904)\n",
    "features = [\"fare_amount\", \"passenger_count\", \"VendorID\", \"trip_distance\"]\n",
    "spark_va = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "yellow_sm = spark_va.transform(yellow_sm)\n",
    "print(\"Number of records: %d\"%yellow_sm.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.38 ms, sys: 322 µs, total: 8.7 ms\n",
      "Wall time: 97.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "import pyspark.ml as sml\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "spark_lasso0 = LinearRegression( \n",
    "    elasticNetParam=1.0, #set it as a Lasso L1\n",
    "    labelCol=\"tip_amount\",\n",
    "    predictionCol=\"y_hat\"\n",
    ")\n",
    "# spark_lasso0m = spark_lasso0.fit(yellow_sm )\n",
    "\n",
    "spark_grid = ParamGridBuilder() \\\n",
    "    .addGrid(spark_lasso0.tol, [1e-3, 1e-4, 1e-5]) \\\n",
    "    .addGrid(spark_lasso0.epsilon, [1.35, 1.70, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "spark_eval = RegressionEvaluator(predictionCol=\"y_hat\", labelCol=\"tip_amount\")\n",
    "spark_cv1 = CrossValidator(estimator=spark_lasso0,\n",
    "                     evaluator=spark_eval,\n",
    "                     estimatorParamMaps=spark_grid, parallelism=20)\n",
    "spark_pipe = sml.Pipeline(stages=[spark_cv1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 3.4 ms, total: 13.8 ms\n",
      "Wall time: 718 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train & test sets\n",
    "yellow_train, yellow_test = yellow_sm.randomSplit(weights=[0.7, 0.3], seed=175904)\n",
    "yellow_train, yellow_test = (yellow_train.cache(), yellow_test.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records\n",
      "Full set:  94890 (100%) \n",
      "Train set: 66539 (~60%)\n",
      "Test set:  28351 (~40%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records\\nFull set:  %d (100%%) \\nTrain set: %d (~60%%)\\nTest set:  %d (~40%%)\"%\n",
    "      (yellow_sm.count(), yellow_train.count(), yellow_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+-----------------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|   pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|label|         features|             y_hat|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+-----------------+------------------+\n",
      "|       1| 2015-01-22 22:40:50|  2015-01-22 22:51:50|              1|          1.4|-73.987197875976563|40.729305267333984|         1|                 N|-74.005867004394531|40.740413665771484|           1|        9.0|  0.5|    0.5|       1.0|           0|                  0.3|        11.3|  1.0|[9.0,1.0,1.0,1.4]|1.0947698299155924|\n",
      "|       1| 2015-01-22 22:55:37|  2015-01-22 23:01:13|              1|          1.1|-73.992034912109375|40.744003295898438|         1|                 N|-73.978744506835938|40.753261566162109|           1|        6.0|  0.5|    0.5|      1.45|           0|                  0.3|        8.75| 15.0|[6.0,1.0,1.0,1.1]|0.8699415943395896|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+-----------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 2.07 s, sys: 712 ms, total: 2.78 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark_pipem = spark_pipe.fit(yellow_sm)\n",
    "spark_preds = spark_pipem.transform(yellow_sm)\n",
    "\n",
    "spark_preds.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.601646 with metric 'rmse'\n",
      "Best parameters for grid search:\n",
      "{\"epsilon\":2.00, \"tol\":0.0001}\n"
     ]
    }
   ],
   "source": [
    "ev = spark_pipe.getStages()[0].getEvaluator()\n",
    "#obtenemos la métrica del error\n",
    "metric = ev.getMetricName()\n",
    "#obtenemos el valor del eror\n",
    "error  = ev.evaluate(spark_preds)\n",
    "best_params = [spark_pipem.stages[0].bestModel.extractParamMap()[x] \n",
    "               for x in [spark_lasso0.epsilon, spark_lasso0.tol]]\n",
    "print(\"Score: %f with metric '%s'\\n\"%(error, metric) +\n",
    "      \"Best parameters for grid search:\\n\" +\n",
    "      '{\"epsilon\":%.2f, \"tol\":%.4f}'%(best_params[0], best_params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de Gran Escala\n",
    "\n",
    "Ahora se utilizarán todos los datos (>1 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 708 ms, total: 2.97 s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "yellow2 = spark_va.transform(yellow)\n",
    "#train & test sets\n",
    "yellow_train2, yellow_test2 = yellow2.randomSplit(weights=[0.6, 0.4], seed=175904)\n",
    "yellow_train2, yellow_test2 = (yellow_train2.cache(), yellow_test2.cache())\n",
    "spark_pipe2m = spark_pipe.fit(yellow_train2)\n",
    "spark_preds2 = spark_pipe2m.transform(yellow_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records\n",
      "Full set:  12748986 (100%) \n",
      "Train set: 7650508 (~60%)\n",
      "Test set:  5098478 (~40%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records\\nFull set:  %d (100%%) \\nTrain set: %d (~60%%)\\nTest set:  %d (~40%%)\"%\n",
    "      (yellow2.count(), yellow_train2.count(), yellow_test2.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.332589 with metric 'rmse'\n",
      "Best parameters for grid search:\n",
      "{\"epsilon\":1.35, \"tol\":0.0010}\n"
     ]
    }
   ],
   "source": [
    "ev2 = spark_pipe.getStages()[0].getEvaluator()\n",
    "#obtenemos el valor del eror\n",
    "error2  = ev2.evaluate(spark_preds2)\n",
    "best_params2 = [spark_pipe2m.stages[0].bestModel.extractParamMap()[x] \n",
    "               for x in [spark_lasso0.epsilon, spark_lasso0.tol]]\n",
    "print(\"Score: %f with metric '%s'\\n\"%(error2, metric) +\n",
    "      \"Best parameters for grid search:\\n\" +\n",
    "      '{\"epsilon\":%.2f, \"tol\":%.4f}'%(best_params2[0], best_params2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+------------------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|   pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|label|          features|             y_hat|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+------------------+------------------+\n",
      "|       1| 2015-01-01 00:11:42|  2015-01-01 00:23:57|              2|          0.9|-73.980201721191406|40.743328094482422|         1|                 N|-73.986419677734375|40.751033782958984|           2|        8.5|  0.5|    0.5|       0.0|           0|                    0|         9.8|  0.0| [8.5,2.0,1.0,0.9]|0.7141605091519161|\n",
      "|       1| 2015-01-01 00:11:42|  2015-01-01 00:26:45|              2|          1.8| -73.99090576171875|40.751144409179688|         1|                 N|-73.988945007324219|40.731418609619141|           1|       10.7|  0.5|    0.5|       2.4|           0|                    0|        14.4| 41.0|[10.7,2.0,1.0,1.8]|1.0961311441998318|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_preds2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Cambia alguno de los resultados anteriores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Respuesta: Los errores ahora son mucho mayores con el dataset de S3, investigando encontré que pudiera ser porque existen muchos más outliers. Esto es importante, dado que esta muestra es mayor, y seguramente recoge mejor la realidad. Entonces sería importante analizar más este dataset para encontrar las razones detrás de esto, y evaluar si otros modelos tienen mejores resultados._\n",
    "\n",
    "_También se observa que los tiempos de ejecución son similares para datos pequeños que para grandes datasets. Eso es importante, como se detallo al inicio de esta sección._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Encontré muy interesante utilizar Dask y Dask-ML. Es relevante que utilice **scikit-learn** dado que tiene muchos más modelos y es más maduro y controlable que Spark. \n",
    "\n",
    "Me encontré con otras otras situaciones al hacer mi cluster _on-prem_ dado que esta maestría me está siendo pagado por la empresa en la que laboro. Dicha empresa no tiene permitido utilizar servicios en la nube por regulación interna. \n",
    "\n",
    "Por lo tanto, dediqué mucho tiempo y esfuerzo, y me topé, con muchos problemas para utilizar S3 dentro de Hadoop y Spark dado que las dependencias de Java en Spark+Hadoop son estrictas y en palabras de otro AWS-SDK es _frágil_. Ejecuté toda esta tarea en un cluster de 3 nodos, cada uno con 64 GiB de RAM; con 16 vCPUs (el master) y con 8 vCPUs (2 workers/esclavos). En todos los casos no se consumía más del 50% de RAM, por lo que correr Dask y Spark _side by side_ no afectaba el rendimiento del uno al otro en ninguno de los nodos. Estos nodos estában dedicados únicamente para este propósito, aunque conviven con otras virtuales con utilización suficiente en distintos nodos **sin sobre suscripción**. Por lo que estas pruebas fueron realizadas relativamente en un ambiente suficientemente aislado y controlado, a diferencia de AWS EC2+EMR, que en ocasiones los jobs corren en hosts sobresuscritos o con cargas pesadas de otros  clientes que afectan las nuestras.\n",
    "\n",
    "Así mismo, EMR hace las cosas muy fácil y _out of the box_, por lo que configurar Spark fue complejo.\n",
    "\n",
    "También me encontré con que Spark no está distribuyendo homogéneamente la carga: los workers con 8 vCPUs no tenían gran utilización (y entiendo) de los recursos de memoria y procesamiento. Necesito investigar porqué está ocurriendo. Pero aún así lo encontré sumamente ágil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "* <http://dask-ml.readthedocs.io/en/latest/>\n",
    "* <https://stackoverflow.com/questions/44167038/subset-dask-dataframe-by-column-position>\n",
    "* <https://stackoverflow.com/questions/39721800/convert-pandas-dataframe-to-dask-dataframe>\n",
    "* <https://dask-ml.readthedocs.io/en/latest/examples/predict.html>\n",
    "* <https://mapr.com/blog/predicting-breast-cancer-using-apache-spark-machine-learning-logistic-regression/>\n",
    "* <https://en.wikipedia.org/wiki/Root-mean-square_deviation>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
